{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sda.encoder_image import Encoder\n",
    "from sda.img_generator import Generator\n",
    "from sda.rnn_audio import RNN\n",
    "from sda.encoder_audio import Encoder as AEncoder\n",
    "\n",
    "from scipy import signal\n",
    "from skimage import transform as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import shutil\n",
    "import skvideo.io as sio\n",
    "import scipy.io.wavfile as wav\n",
    "import ffmpeg\n",
    "import face_alignment\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "import glob\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "dev = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/jarrod/dev/speech-driven-animation/sda/data/grid.dat\"\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(0))\n",
    "model_dict = torch.load(model_path, map_location=lambda storage, loc: storage.cuda(0))\n",
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cpu\", flip_input=False)\n",
    "\n",
    "stablePntsIDs = [33, 36, 39, 42, 45]\n",
    "mean_face = model_dict[\"mean_face\"]\n",
    "img_size = model_dict[\"img_size\"]\n",
    "audio_rate = model_dict[\"audio_rate\"]\n",
    "video_rate = model_dict[\"video_rate\"]\n",
    "audio_feat_len = model_dict['audio_feat_len']\n",
    "audio_feat_samples = model_dict['audio_feat_samples']\n",
    "id_enc_dim = model_dict['id_enc_dim']\n",
    "rnn_gen_dim = model_dict['rnn_gen_dim']\n",
    "aud_enc_dim = model_dict['aud_enc_dim']\n",
    "# I think this is the size of the noise vector\n",
    "aux_latent = model_dict['aux_latent']\n",
    "# sequential noise is a boolean value\n",
    "sequential_noise = model_dict['sequential_noise']\n",
    "conversion_dict = {'s16': np.int16, 's32': np.int32}\n",
    "        \n",
    "# image preprocessing\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((img_size[0], img_size[1])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "def preprocess_img(img):\n",
    "        src = fa.get_landmarks(img)[0][stablePntsIDs, :]\n",
    "        dst = mean_face[stablePntsIDs, :]\n",
    "        tform = tf.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "        warped = tf.warp(img, inverse_map=tform.inverse, output_shape=img_size)  # wrap the frame image\n",
    "        warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "        warped = warped.astype('uint8')\n",
    "\n",
    "        return warped\n",
    "\n",
    "def _cut_sequence_(seq, cutting_stride, pad_samples):\n",
    "    pad_left = torch.zeros(pad_samples // 2, 1)\n",
    "    pad_right = torch.zeros(pad_samples - pad_samples // 2, 1)\n",
    "\n",
    "    seq = torch.cat((pad_left, seq), 0)\n",
    "    seq = torch.cat((seq, pad_right), 0)\n",
    "\n",
    "    stacked = seq.narrow(0, 0, audio_feat_samples).unsqueeze(0)\n",
    "    iterations = (seq.size()[0] - audio_feat_samples) // cutting_stride + 1\n",
    "    for i in range(1, iterations):\n",
    "        stacked = torch.cat((stacked, seq.narrow(0, i * cutting_stride, audio_feat_samples).unsqueeze(0)))\n",
    "    return stacked#.to(self.device)\n",
    "\n",
    "def genSample(img, audio, fs=None, aligned=False):\n",
    "        if isinstance(img, str):  # if we have a path then grab the image\n",
    "            frm = Image.open(img)\n",
    "            frm.thumbnail((400, 400))\n",
    "            frame = np.array(frm)\n",
    "        else:\n",
    "            frame = img\n",
    "\n",
    "        # handle aligning the face with the model's learned \"mean face\"\n",
    "        # may also do some preprocessing\n",
    "        if not aligned:\n",
    "            frame = preprocess_img(frame)\n",
    "\n",
    "        # if we have a path then grab the audio clip\n",
    "        if isinstance(audio, str):  \n",
    "            info = mediainfo(audio)\n",
    "            fs = int(info['sample_rate'])\n",
    "            audio = np.array(AudioSegment.from_file(audio, info['format_name']).set_channels(1).get_array_of_samples())\n",
    "\n",
    "            if info['sample_fmt'] in conversion_dict:\n",
    "                audio = audio.astype(conversion_dict[info['sample_fmt']])\n",
    "            else:\n",
    "                if max(audio) > np.iinfo(np.int16).max:\n",
    "                    audio = audio.astype(np.int32)\n",
    "                else:\n",
    "                    audio = audio.astype(np.int16)\n",
    "\n",
    "        if fs is None:\n",
    "            raise AttributeError(\"Audio provided without specifying the rate. Specify rate or use audio file!\")\n",
    "\n",
    "        if audio.ndim > 1 and audio.shape[1] > 1:\n",
    "            audio = audio[:, 0]\n",
    "\n",
    "        max_value = np.iinfo(audio.dtype).max\n",
    "        \n",
    "        if fs != audio_rate:\n",
    "            seq_length = audio.shape[0]\n",
    "            speech = torch.from_numpy(\n",
    "                signal.resample(audio, int(seq_length * audio_rate / float(fs))) / float(max_value)).float()\n",
    "            speech = speech.view(-1, 1)\n",
    "            \n",
    "        else:\n",
    "            audio = torch.from_numpy(audio / float(max_value)).float()\n",
    "            speech = audio.view(-1, 1)\n",
    "\n",
    "#         take the input image and preprocess it    \n",
    "        frame = img_transform(frame)#.to(self.device)\n",
    "\n",
    "        cutting_stride = int(audio_rate / float(video_rate))\n",
    "        audio_seq_padding = audio_feat_samples - cutting_stride\n",
    "\n",
    "        # Create new sequences of the audio windows\n",
    "        audio_feat_seq = _cut_sequence_(speech, cutting_stride, audio_seq_padding)\n",
    "        frame = frame.unsqueeze(0)\n",
    "        audio_feat_seq = audio_feat_seq.unsqueeze(0)\n",
    "        audio_feat_seq_length = audio_feat_seq.size()[1]\n",
    "    \n",
    "        return speech, audio_feat_seq, audio_feat_seq_length, frame\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, a1, a2, frame = genSample(\"example/male_face2.jpg\", \"example/hello_world.wav\", aligned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, feat_length, enc_code_size, rnn_code_size, rate, n_layers=2, init_kernel=None,\n",
    "                 init_stride=None):\n",
    "        super(RNN, self).__init__()\n",
    "        self.audio_feat_samples = int(rate * feat_length)\n",
    "        self.enc_code_size = enc_code_size\n",
    "        self.rnn_code_size = rnn_code_size\n",
    "        self.encoder = AEncoder(self.enc_code_size, rate, feat_length, init_kernel=init_kernel,\n",
    "                               init_stride=init_stride)\n",
    "        self.rnn = nn.GRU(self.enc_code_size, self.rnn_code_size, n_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        seq_length = x.size()[1]\n",
    "        print(\"encoder11 \", x.requires_grad)\n",
    "        x = x.reshape(-1, 1, self.audio_feat_samples)\n",
    "        print(\"encoder22 \", x.requires_grad)\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, seq_length, self.enc_code_size)\n",
    "        print(\"encoder33 \", x.requires_grad)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "#         print(\"encoder44 \", x.requires_grad)\n",
    "#         print(x.shape)\n",
    "#         print(self.enc_code_size, self.rnn_code_size)\n",
    "        x, h = self.rnn(x)\n",
    "#         print(\"encoder55 \", x.requires_grad)\n",
    "        \n",
    "        x, lengths = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        print(\"encoder66 \", x.requires_grad)\n",
    "        return x.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio encoder\n",
    "# size of noise vector\n",
    "aux_latent = 10\n",
    "sequential_noise = True\n",
    "img_size = (128,96)\n",
    "rnn_gen_dim = 256\n",
    "id_enc_dim = 128\n",
    "aud_enc_dim = 256\n",
    "audio_feat_len = 0.2\n",
    "audio_rate = 50000\n",
    "encoder = RNN(audio_feat_len, aud_enc_dim, rnn_gen_dim, audio_rate, init_kernel=0.005, init_stride=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X = torch.ones((1,71,10000,1))\n",
    "X.requires_grad = True\n",
    "# X = X.to(dev)\n",
    "X.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d5651a1d710f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a1' is not defined"
     ]
    }
   ],
   "source": [
    "a1 = a1.cuda()\n",
    "a1.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder11  True\n",
      "encoder22  True\n",
      "encoder33  True\n",
      "encoder66  True\n"
     ]
    }
   ],
   "source": [
    "a3 = torch.Tensor([10])\n",
    "a3.requires_grad = True\n",
    "z = encoder(X, a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
