{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sda.encoder_image import Encoder\n",
    "from sda.img_generator import Generator\n",
    "from sda.rnn_audio import RNN\n",
    "from sda.encoder_audio import Encoder as AEncoder\n",
    "\n",
    "from scipy import signal\n",
    "from skimage import transform as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import shutil\n",
    "import skvideo.io as sio\n",
    "import scipy.io.wavfile as wav\n",
    "import ffmpeg\n",
    "import face_alignment\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "import glob\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "dev = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = glob.glob('/home/jarrod/dev/speech-driven-animation/data/*/*.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getDataSample():\n",
    "    def __init__(self, model_path=\"grid\", gpu=-1):\n",
    "\n",
    "        if model_path == \"grid\":\n",
    "            model_path = \"/home/jarrod/dev/speech-driven-animation/sda/data/grid.dat\"\n",
    "#         elif model_path == \"timit\":\n",
    "#             model_path = os.path.split(__file__)[0] + \"/data/timit.dat\"\n",
    "#         elif model_path == \"crema\":\n",
    "#             model_path = os.path.split(__file__)[0] + \"/data/crema.dat\"\n",
    "\n",
    "        if gpu < 0:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            model_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "            self.fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cpu\", flip_input=False)\n",
    "        else:\n",
    "            self.device = torch.device(\"cuda:\" + str(gpu))\n",
    "            model_dict = torch.load(model_path, map_location=lambda storage, loc: storage.cuda(gpu))\n",
    "            self.fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cuda:\" + str(gpu),\n",
    "                                                   flip_input=False)\n",
    "\n",
    "        self.stablePntsIDs = [33, 36, 39, 42, 45]\n",
    "        self.mean_face = model_dict[\"mean_face\"]\n",
    "        self.img_size = model_dict[\"img_size\"]\n",
    "        self.audio_rate = model_dict[\"audio_rate\"]\n",
    "        self.video_rate = model_dict[\"video_rate\"]\n",
    "        self.audio_feat_len = model_dict['audio_feat_len']\n",
    "        self.audio_feat_samples = model_dict['audio_feat_samples']\n",
    "        self.id_enc_dim = model_dict['id_enc_dim']\n",
    "        self.rnn_gen_dim = model_dict['rnn_gen_dim']\n",
    "        self.aud_enc_dim = model_dict['aud_enc_dim']\n",
    "        # I think this is the size of the noise vector\n",
    "        self.aux_latent = model_dict['aux_latent']\n",
    "        # sequential noise is a boolean value\n",
    "        self.sequential_noise = model_dict['sequential_noise']\n",
    "        self.conversion_dict = {'s16': np.int16, 's32': np.int32}\n",
    "        \n",
    "        # image preprocessing\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.img_size[0], self.img_size[1])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        src = self.fa.get_landmarks(img)[0][self.stablePntsIDs, :]\n",
    "        dst = self.mean_face[self.stablePntsIDs, :]\n",
    "        tform = tf.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "        warped = tf.warp(img, inverse_map=tform.inverse, output_shape=self.img_size)  # wrap the frame image\n",
    "        warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "        warped = warped.astype('uint8')\n",
    "\n",
    "        return warped\n",
    "\n",
    "    def _cut_sequence_(self, seq, cutting_stride, pad_samples):\n",
    "        pad_left = torch.zeros(pad_samples // 2, 1)\n",
    "        pad_right = torch.zeros(pad_samples - pad_samples // 2, 1)\n",
    "\n",
    "        seq = torch.cat((pad_left, seq), 0)\n",
    "        seq = torch.cat((seq, pad_right), 0)\n",
    "\n",
    "        stacked = seq.narrow(0, 0, self.audio_feat_samples).unsqueeze(0)\n",
    "        iterations = (seq.size()[0] - self.audio_feat_samples) // cutting_stride + 1\n",
    "        for i in range(1, iterations):\n",
    "            stacked = torch.cat((stacked, seq.narrow(0, i * cutting_stride, self.audio_feat_samples).unsqueeze(0)))\n",
    "        return stacked#.to(self.device)\n",
    "\n",
    "    def __call__(self, img, audio_path, fs=None, aligned=False):\n",
    "#         if isinstance(img, str):  # if we have a path then grab the image\n",
    "#             frm = Image.open(img)\n",
    "#             frm.thumbnail((400, 400))\n",
    "#             frame = np.array(frm)\n",
    "#         else:\n",
    "#             frame = img\n",
    "\n",
    "            \n",
    "        # handle aligning the face with the model's learned \"mean face\"\n",
    "        # may also do some preprocessing\n",
    "#         if not aligned:\n",
    "#             frame = self.preprocess_img(frame)\n",
    "\n",
    "        # if we have a path then grab the audio clip\n",
    "        if isinstance(audio_path, str):  \n",
    "            info = mediainfo(audio_path)\n",
    "            fs = int(info['sample_rate'])\n",
    "            audio = np.array(AudioSegment.from_file(audio_path, info['format_name']).set_channels(1).get_array_of_samples())\n",
    "            \n",
    "            if info['sample_fmt'] in self.conversion_dict:\n",
    "                audio = audio.astype(self.conversion_dict[info['sample_fmt']])\n",
    "            else:\n",
    "                if max(audio) > np.iinfo(np.int16).max:\n",
    "                    audio = audio.astype(np.int32)\n",
    "                else:\n",
    "                    audio = audio.astype(np.int16)\n",
    "        \n",
    "            \n",
    "        if fs is None:\n",
    "            raise AttributeError(\"Audio provided without specifying the rate. Specify rate or use audio file!\")\n",
    "\n",
    "        if audio.ndim > 1 and audio.shape[1] > 1:\n",
    "            audio = audio[:, 0]\n",
    "\n",
    "        max_value = np.iinfo(audio.dtype).max\n",
    "        \n",
    "        if fs != self.audio_rate:\n",
    "            seq_length = audio.shape[0]\n",
    "            speech = torch.from_numpy(\n",
    "                signal.resample(audio, int(seq_length * self.audio_rate / float(fs))) / float(max_value)).float()\n",
    "            speech = speech.view(-1, 1)\n",
    "            \n",
    "        else:\n",
    "            audio = torch.from_numpy(audio / float(max_value)).float()\n",
    "            speech = audio.view(-1, 1)\n",
    "            \n",
    "        \n",
    "        # take the input image and preprocess it    \n",
    "#         frame = self.img_transform(frame)#.to(self.device)\n",
    "\n",
    "        cutting_stride = int(self.audio_rate / float(self.video_rate))\n",
    "        audio_seq_padding = self.audio_feat_samples - cutting_stride\n",
    "\n",
    "        # Create new sequences of the audio windows\n",
    "        audio_feat_seq = self._cut_sequence_(speech, cutting_stride, audio_seq_padding)\n",
    "#         frame = frame.unsqueeze(0)\n",
    "        audio_feat_seq = audio_feat_seq.unsqueeze(0)\n",
    "        audio_feat_seq_length = audio_feat_seq.size()[1]\n",
    "        \n",
    "        out = audio_feat_seq.numpy()\n",
    "        np.save('./data/npy_audio/' + audio_path.split('/')[-1] + '.npy', out)\n",
    "    \n",
    "        return #speech, audio_feat_seq, audio_feat_seq_length, frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = getDataSample(gpu=0, model_path=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_filenames))\n",
    "count = 0\n",
    "\n",
    "for f in audio_filenames:\n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "    va(\"example/male_face2.jpg\", f, aligned=True)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cuda:\" + str(0), flip_input=False)\n",
    "mean_face = np.load('./data/mean_face.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    stablePntsIDs = [33, 36, 39, 42, 45]\n",
    "    \n",
    "    src = fa.get_landmarks(img)\n",
    "    if src != None:\n",
    "        dst = mean_face[stablePntsIDs, :]\n",
    "        tform = tf.estimate_transform('similarity', src[0][stablePntsIDs, :], dst)  # find the transformation matrix\n",
    "        warped = tf.warp(img, inverse_map=tform.inverse, output_shape=(128,96))  # wrap the frame image\n",
    "        warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "        warped = warped.astype('uint8')\n",
    "\n",
    "    else:\n",
    "        warped = np.zeros((128,96,3))\n",
    "        \n",
    "    \n",
    "    \n",
    "    return warped\n",
    "\n",
    "def cropFace(frame):\n",
    "    \n",
    "    src = fa.get_landmarks(frame)\n",
    "    err = 0\n",
    "    \n",
    "    if src != None and src[0].shape[0] == 68 and src[0].shape[1] == 2:\n",
    "    \n",
    "        max_x = int(np.max(src[0][:,0]))\n",
    "        min_x = int(np.min(src[0][:,0]))\n",
    "\n",
    "        max_y = int(np.max(src[0][:,1]))\n",
    "        min_y = int(np.min(src[0][:,1]))\n",
    "\n",
    "        center_x = int(min_x + (max_x - min_x)/2)\n",
    "        center_y = int(min_y + (max_y - min_y)/2)\n",
    "\n",
    "        img_height = 128\n",
    "        img_width = 96\n",
    "\n",
    "        left_crop = int(center_x - img_width/2)\n",
    "        right_crop = int(center_x + img_width/2)\n",
    "\n",
    "        top_crop = int(center_y - img_height/2)\n",
    "        bottom_crop = int(center_y + img_height/2)\n",
    "\n",
    "        crop = frame[top_crop:bottom_crop, left_crop:right_crop, :]\n",
    "        \n",
    "#         print(\"crop \", crop.shape)\n",
    "    \n",
    "    else:\n",
    "        err = 1\n",
    "        crop = 0\n",
    "    \n",
    "    return crop, err\n",
    "\n",
    "def alignFace(vid_data, fname):\n",
    "    \n",
    "    new_vid = []\n",
    "    \n",
    "    for i, frame in enumerate(vid_data):\n",
    "          \n",
    "        crop, err = cropFace(frame)\n",
    "        \n",
    "#         print(\"err\", err)\n",
    "#         if err != 1:\n",
    "#             print(\"2nd crop \", crop.shape)\n",
    "        \n",
    "        if err == 0:\n",
    "#             print(\"immediately before\", crop.shape)\n",
    "            new_vid.append(preprocess_img(crop))\n",
    "        else:\n",
    "            print(\"error in \", fname, \" at frame \", i)\n",
    "    \n",
    "    err = 0\n",
    "    \n",
    "    if len(new_vid) < 1:\n",
    "        err = 1\n",
    "        out = None\n",
    "    else:\n",
    "        out = np.stack(new_vid)\n",
    "    \n",
    "    return out, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbv9a.wav\n",
      "0  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbat9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrwr9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrwl5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwbn5a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbwl1a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgan7a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgwo3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbbuzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbbh2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbac2p.wav\n",
      "10  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srau2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbp4p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgbh6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbwr2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbi9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srwvzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgwx2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbwu4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swab8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwanzp.wav\n",
      "20  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/brwa3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrbl3a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pric4p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgis1a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwip6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwbt7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbwm6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgwr4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwaj9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbav2n.wav\n",
      "30  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgahzn.wav\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# for f in audio_filenames:\n",
    "    \n",
    "    \n",
    "for f in range(2600,3000):\n",
    "    \n",
    "    f = audio_filenames[f]\n",
    "    print(\"file: \", f)\n",
    "    if count % 10 == 0:\n",
    "        print(count, \" of \" + str(len(audio_filenames)))\n",
    "    vid_data = sio.vread(f.split('.wav')[0] + \".mpg\")\n",
    "    out, err = alignFace(vid_data, f.split('.wav')[0] + \".mpg\")\n",
    "    \n",
    "    if err != 1:\n",
    "        np.save('./data/aligned_faces/' + f.split('/')[-1].split('.wav')[0] + \".npy\", out)\n",
    "    else:\n",
    "        print(\"error with video \", vid_data, f.split('.wav')[0] + \".mpg\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
