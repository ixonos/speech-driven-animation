{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from moviepy.editor import *\n",
    "\n",
    "from sda.encoder_image import Encoder\n",
    "from sda.img_generator import Generator\n",
    "from sda.rnn_audio import RNN\n",
    "from sda.encoder_audio import Encoder as AEncoder\n",
    "\n",
    "from scipy import signal\n",
    "from skimage import transform as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import shutil\n",
    "import skvideo.io as sio\n",
    "import scipy.io.wavfile as wav\n",
    "import ffmpeg\n",
    "import face_alignment\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "import glob\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "dev = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = glob.glob('/home/jarrod/dev/speech-driven-animation/data/*/*.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getDataSample():\n",
    "    def __init__(self, model_path=\"grid\", gpu=-1):\n",
    "\n",
    "#         if model_path == \"grid\":\n",
    "#             model_path = \"/home/jarrod/dev/speech-driven-animation/sda/data/grid.dat\"\n",
    "# #         elif model_path == \"timit\":\n",
    "# #             model_path = os.path.split(__file__)[0] + \"/data/timit.dat\"\n",
    "# #         elif model_path == \"crema\":\n",
    "# #             model_path = os.path.split(__file__)[0] + \"/data/crema.dat\"\n",
    "\n",
    "#         if gpu < 0:\n",
    "#             self.device = torch.device(\"cpu\")\n",
    "#             model_dict = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "#             self.fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cpu\", flip_input=False)\n",
    "#         else:\n",
    "#             self.device = torch.device(\"cuda:\" + str(gpu))\n",
    "#             model_dict = torch.load(model_path, map_location=lambda storage, loc: storage.cuda(gpu))\n",
    "#             self.fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cuda:\" + str(gpu),\n",
    "#                                                    flip_input=False)\n",
    "\n",
    "#         self.stablePntsIDs = [33, 36, 39, 42, 45]\n",
    "#         self.mean_face = model_dict[\"mean_face\"]\n",
    "#         self.img_size = model_dict[\"img_size\"]\n",
    "#         self.audio_rate = model_dict[\"audio_rate\"]\n",
    "#         self.video_rate = model_dict[\"video_rate\"]\n",
    "#         self.audio_feat_len = model_dict['audio_feat_len']\n",
    "#         self.audio_feat_samples = model_dict['audio_feat_samples']\n",
    "#         self.id_enc_dim = model_dict['id_enc_dim']\n",
    "#         self.rnn_gen_dim = model_dict['rnn_gen_dim']\n",
    "#         self.aud_enc_dim = model_dict['aud_enc_dim']\n",
    "#         # I think this is the size of the noise vector\n",
    "#         self.aux_latent = model_dict['aux_latent']\n",
    "#         # sequential noise is a boolean value\n",
    "#         self.sequential_noise = model_dict['sequential_noise']\n",
    "#    self.conversion_dict = {'s16': np.int16, 's32': np.int32}\n",
    "     \n",
    "        self.mean_face = np.load('./data/mean_face.npy')\n",
    "        self.img_size = (128,96)\n",
    "        self.aux_latent = 10\n",
    "        self.sequential_noise = True\n",
    "    \n",
    "        rnn_gen_dim = 256\n",
    "        id_enc_dim = 128\n",
    "        aud_enc_dim = 256\n",
    "        audio_feat_len = 0.2\n",
    "        self.audio_rate = 50000\n",
    "        self.video_rate = 25\n",
    "        self.audio_feat_samples = 10000\n",
    "        self.conversion_dict = {'s16': np.int16, 's32': np.int32}\n",
    "     \n",
    "        \n",
    "        # image preprocessing\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.img_size[0], self.img_size[1])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        src = self.fa.get_landmarks(img)[0][self.stablePntsIDs, :]\n",
    "        dst = self.mean_face[self.stablePntsIDs, :]\n",
    "        tform = tf.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "        warped = tf.warp(img, inverse_map=tform.inverse, output_shape=self.img_size)  # wrap the frame image\n",
    "        warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "        warped = warped.astype('uint8')\n",
    "\n",
    "        return warped\n",
    "\n",
    "    def _cut_sequence_(self, seq, cutting_stride, pad_samples):\n",
    "        pad_left = torch.zeros(pad_samples // 2, 1) # = 4000\n",
    "        pad_right = torch.zeros(pad_samples - pad_samples // 2, 1) # = 4000\n",
    "\n",
    "        seq = torch.cat((pad_left, seq), 0)\n",
    "        seq = torch.cat((seq, pad_right), 0) # = (158,000, 1)\n",
    "\n",
    "        stacked = seq.narrow(0, 0, self.audio_feat_samples).unsqueeze(0) # = (1,10000,1)\n",
    "        \n",
    "        iterations = (seq.size()[0] - self.audio_feat_samples) // cutting_stride + 1\n",
    "        for i in range(1, iterations):\n",
    "            stacked = torch.cat((stacked, seq.narrow(0, i * cutting_stride, self.audio_feat_samples).unsqueeze(0)))\n",
    "        return stacked#.to(self.device)\n",
    "\n",
    "    def __call__(self, img, audio_path, fs=None, aligned=False):\n",
    "\n",
    "        # if we have a path then grab the audio clip\n",
    "        if isinstance(audio_path, str):  \n",
    "            \n",
    "            video = VideoFileClip(audio_path.split('.wav')[0] + '.mpg')\n",
    "            audio = video.audio\n",
    "                        \n",
    "            if audio.fps != 44100:\n",
    "                print(\"audio Sampling rate is not 44.1khz\")\n",
    "            \n",
    "            fs = audio.fps\n",
    "            audio = audio.to_soundarray(fps=fs).mean(axis=1).astype(np.float32)\n",
    "\n",
    "        # exlude this part: audio loader loads to float normalized on [-1,1]\n",
    "#         if max(audio) > np.iinfo(np.int16).max:\n",
    "#             audio = audio.astype(np.int32)\n",
    "#         else:\n",
    "#             audio = audio.astype(np.int16)\n",
    "\n",
    "\n",
    "#         max_value = np.iinfo(audio.dtype).max\n",
    "        \n",
    "        # resample the audio clip to 50khz to make the chopping math easy i.e. 2000 samples per frame at 25 fps\n",
    "        if fs != self.audio_rate:\n",
    "            \n",
    "            seq_length = audio.shape[0]\n",
    "            speech = torch.from_numpy(\n",
    "                signal.resample(audio, int(seq_length * self.audio_rate / float(fs)))) #/ float(max_value)).float()\n",
    "            speech = speech.view(-1, 1).float()\n",
    "            \n",
    "        else:\n",
    "            audio = torch.from_numpy(audio / float(max_value)).float()\n",
    "            speech = audio.view(-1, 1).float()\n",
    "\n",
    "        cutting_stride = int(self.audio_rate / float(self.video_rate))\n",
    "     \n",
    "        audio_seq_padding = self.audio_feat_samples - cutting_stride\n",
    "\n",
    "        # Create new sequences of the audio windows\n",
    "        audio_feat_seq = self._cut_sequence_(speech, cutting_stride, audio_seq_padding)\n",
    "        audio_feat_seq = audio_feat_seq.unsqueeze(0)\n",
    "        audio_feat_seq_length = audio_feat_seq.size()[1]\n",
    "        \n",
    "#         print(np.min(audio_feat_seq.cpu().detach().numpy()), audio_feat_seq.shape)\n",
    "        \n",
    "        out = audio_feat_seq.numpy()\n",
    "        \n",
    "        np.save('./data/npy_audio/' + audio_path.split('/')[-1] + '.npy', out)\n",
    "    \n",
    "        return #speech, audio_feat_seq, audio_feat_seq_length, frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = getDataSample(gpu=0, model_path=\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud = va(\"example/male_face2.jpg\", audio_filenames[0], aligned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "print(len(audio_filenames))\n",
    "count = 0\n",
    "\n",
    "for f in audio_filenames:\n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "        \n",
    "    va(\"example/male_face2.jpg\", f, aligned=True)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, device=\"cuda:\" + str(0), flip_input=False)\n",
    "mean_face = np.load('./data/mean_face.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    stablePntsIDs = [33, 36, 39, 42, 45]\n",
    "    \n",
    "    src = fa.get_landmarks(img)\n",
    "    if src != None:\n",
    "        dst = mean_face[stablePntsIDs, :]\n",
    "        tform = tf.estimate_transform('similarity', src[0][stablePntsIDs, :], dst)  # find the transformation matrix\n",
    "        warped = tf.warp(img, inverse_map=tform.inverse, output_shape=(128,96))  # wrap the frame image\n",
    "        warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "        warped = warped.astype('uint8')\n",
    "\n",
    "    else:\n",
    "        warped = np.zeros((128,96,3))\n",
    "        \n",
    "    \n",
    "    \n",
    "    return warped\n",
    "\n",
    "def cropFace(frame):\n",
    "    \n",
    "    src = fa.get_landmarks(frame)\n",
    "    err = 0\n",
    "    \n",
    "    if src != None and src[0].shape[0] == 68 and src[0].shape[1] == 2:\n",
    "    \n",
    "        max_x = int(np.max(src[0][:,0]))\n",
    "        min_x = int(np.min(src[0][:,0]))\n",
    "\n",
    "        max_y = int(np.max(src[0][:,1]))\n",
    "        min_y = int(np.min(src[0][:,1]))\n",
    "\n",
    "        center_x = int(min_x + (max_x - min_x)/2)\n",
    "        center_y = int(min_y + (max_y - min_y)/2)\n",
    "\n",
    "        img_height = 128\n",
    "        img_width = 96\n",
    "\n",
    "        left_crop = int(center_x - img_width/2)\n",
    "        right_crop = int(center_x + img_width/2)\n",
    "\n",
    "        top_crop = int(center_y - img_height/2)\n",
    "        bottom_crop = int(center_y + img_height/2)\n",
    "\n",
    "        crop = frame[top_crop:bottom_crop, left_crop:right_crop, :]\n",
    "        \n",
    "#         print(\"crop \", crop.shape)\n",
    "    \n",
    "    else:\n",
    "        err = 1\n",
    "        crop = 0\n",
    "    \n",
    "    return crop, err\n",
    "\n",
    "def alignFace(vid_data, fname):\n",
    "    \n",
    "    new_vid = []\n",
    "    \n",
    "    for i, frame in enumerate(vid_data):\n",
    "          \n",
    "        crop, err = cropFace(frame)\n",
    "        \n",
    "#         print(\"err\", err)\n",
    "#         if err != 1:\n",
    "#             print(\"2nd crop \", crop.shape)\n",
    "        \n",
    "        if err == 0:\n",
    "#             print(\"immediately before\", crop.shape)\n",
    "            new_vid.append(preprocess_img(crop))\n",
    "        else:\n",
    "            print(\"error in \", fname, \" at frame \", i)\n",
    "    \n",
    "    err = 0\n",
    "    \n",
    "    if len(new_vid) < 1:\n",
    "        err = 1\n",
    "        out = None\n",
    "    else:\n",
    "        out = np.stack(new_vid)\n",
    "    \n",
    "    return out, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished up to 2730 of 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbv9a.wav\n",
      "0  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbat9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrwr9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrwl5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwbn5a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbwl1a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgan7a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgwo3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbbuzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbbh2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbac2p.wav\n",
      "10  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srau2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbp4p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgbh6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbwr2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbi9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srwvzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgwx2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbwu4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swab8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwanzp.wav\n",
      "20  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/brwa3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrbl3a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pric4p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgis1a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwip6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwbt7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbwm6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgwr4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwaj9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbav2n.wav\n",
      "30  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgahzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgix9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbwo1s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwal3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbid3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bram2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/briz9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sban4p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwafzp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgbrzn.wav\n",
      "40  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrar2p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwik8n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgaq8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgbs9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgin3a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbi8n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwbz7a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgbs6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srbb5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbby4p.wav\n",
      "50  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgwl1s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwixzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgap2p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwwm1s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srwb8n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwwk6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrblzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgbc8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prwd6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwaa1s.wav\n",
      "60  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prwd7a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swwv7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swib3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwwmzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prwx7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgwm9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbib9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/brbg2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swab6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbwg3a.wav\n",
      "70  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/briz6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgbn9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbv6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sriuzp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwbz4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgwh9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbayzp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prbx5a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgah2p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgbm5a.wav\n",
      "80  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgwtzn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prwkzp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbbrzp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgwq1a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgaz7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbwp8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lray5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgiq5a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swwp2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbad9a.wav\n",
      "90  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prwq5a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwar8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrak7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrwz2n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pwbq5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prbj4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbwu5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srwczp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgik1a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/braf8n.wav\n",
      "100  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgak4p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/priv6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgak5a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbaz7a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgay1s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/prac6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/brbm8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swaizn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrwl6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sraozp.wav\n",
      "110  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/brif5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbal6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bris3s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbbn8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgaz6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwbt6n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bwig2p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sbwo2p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bril9s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbaz6p.wav\n",
      "120  of 3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lbbk4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbaz5s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgig8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/brbm7s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lwiy8p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lgil6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbaszn.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgbj1s.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pgwzzp.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/pbbc7a.wav\n",
      "130  of 3000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bgwb4n.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/sgbc9a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/bbbm2p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/lrbr7a.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/srah6p.wav\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/s1/swbczn.wav\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0189eb106119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_filenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".mpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".mpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e2f57d27b8db>\u001b[0m in \u001b[0;36malignFace\u001b[0;34m(vid_data, fname)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#             print(\"immediately before\", crop.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mnew_vid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error in \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" at frame \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e2f57d27b8db>\u001b[0m in \u001b[0;36mpreprocess_img\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstablePntsIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m36\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_face\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstablePntsIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.1/lib/python3.7/site-packages/face_alignment-1.0.0-py3.7.egg/face_alignment/api.py\u001b[0m in \u001b[0;36mget_landmarks\u001b[0;34m(self, image_or_path, detected_faces)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \"\"\"\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_landmarks_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetected_faces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_landmarks_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetected_faces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.1/lib/python3.7/site-packages/face_alignment-1.0.0-py3.7.egg/face_alignment/api.py\u001b[0m in \u001b[0;36mget_landmarks_from_image\u001b[0;34m(self, image_or_path, detected_faces)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_preds_fromhm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0mpts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m68\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpts_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m68\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.1/lib/python3.7/site-packages/face_alignment-1.0.0-py3.7.egg/face_alignment/utils.py\u001b[0m in \u001b[0;36mget_preds_fromhm\u001b[0;34m(hm, center, scale)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 preds_orig[i, j] = transform(\n\u001b[0;32m--> 171\u001b[0;31m                     preds[i, j], center, scale, hm.size(2), True)\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_orig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.1/lib/python3.7/site-packages/face_alignment-1.0.0-py3.7.egg/face_alignment/utils.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(point, center, scale, resolution, invert)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolution\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolution\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolution\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "# for f in audio_filenames:\n",
    "    \n",
    "    \n",
    "for f in range(2600,3000):\n",
    "    \n",
    "    f = audio_filenames[f]\n",
    "    print(\"file: \", f)\n",
    "    if count % 10 == 0:\n",
    "        print(count, \" of \" + str(len(audio_filenames)))\n",
    "    vid_data = sio.vread(f.split('.wav')[0] + \".mpg\")\n",
    "    out, err = alignFace(vid_data, f.split('.wav')[0] + \".mpg\")\n",
    "    \n",
    "    if err != 1:\n",
    "        np.save('./data/aligned_faces/' + f.split('/')[-1].split('.wav')[0] + \".npy\", out)\n",
    "    else:\n",
    "        print(\"error with video \", vid_data, f.split('.wav')[0] + \".mpg\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jarrod/dev/speech-driven-animation/data/s1/swwv8p.wav'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swwv8p.npy'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.split('/')[-1].split('.wav')[0] + \".npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/brwa4p.npy\n",
      "1\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pwbd6s.npy\n",
      "2\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/sbaa4p.npy\n",
      "3\n",
      "200\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/brwg8p.npy\n",
      "4\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/bgbn9a.npy\n",
      "5\n",
      "300\n",
      "400\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pbwxzs.npy\n",
      "6\n",
      "500\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/bwwuzn.npy\n",
      "7\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/bramzn.npy\n",
      "8\n",
      "600\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/bgit2n.npy\n",
      "9\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/srwi5a.npy\n",
      "10\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lrarzn.npy\n",
      "11\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/swao7a.npy\n",
      "12\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/srbb4n.npy\n",
      "13\n",
      "1100\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/sbbbzp.npy\n",
      "14\n",
      "1200\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pgwy7s.npy\n",
      "15\n",
      "1300\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/swiu2n.npy\n",
      "16\n",
      "1400\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lbij7a.npy\n",
      "17\n",
      "1500\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/bbizzn.npy\n",
      "18\n",
      "1600\n",
      "1700\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lrbr3s.npy\n",
      "19\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pbwx1s.npy\n",
      "20\n",
      "1800\n",
      "1900\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lgbz9s.npy\n",
      "21\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lrae3s.npy\n",
      "22\n",
      "2000\n",
      "2100\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/sran9s.npy\n",
      "23\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/sbim8p.npy\n",
      "24\n",
      "2500\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/sgwp8p.npy\n",
      "25\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/sbbh4p.npy\n",
      "26\n",
      "2600\n",
      "2700\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lbij5s.npy\n",
      "27\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pbio7a.npy\n",
      "28\n",
      "2800\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pbib7p.npy\n",
      "29\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/prwq3a.npy\n",
      "30\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/pbiu6n.npy\n",
      "31\n",
      "2900\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/lgbf8n.npy\n",
      "32\n",
      "file:  /home/jarrod/dev/speech-driven-animation/data/aligned_faces/prii9a.npy\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "# Check How Many Videos Have Less Than 75 Frames\n",
    "\n",
    "count = 0\n",
    "\n",
    "# for f in audio_filenames:\n",
    "audio_filenames = glob.glob('/home/jarrod/dev/speech-driven-animation/data/aligned_faces/*.npy') \n",
    "   \n",
    "bad_list = []\n",
    "    \n",
    "for i, f in enumerate(audio_filenames):\n",
    "    \n",
    "    \n",
    "    vid_data = np.load(f)\n",
    "     \n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "\n",
    "\n",
    "    if vid_data.shape[0] < 75:\n",
    "        print(\"file: \", f)\n",
    "        count += 1\n",
    "        print(count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_data = sio.vread('./data/s1/bbizzn.mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 288, 360, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load('./data/aligned_faces/bbizzn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 128, 96, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  0\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  1\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  2\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  3\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  4\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  5\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  6\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  7\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  8\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  9\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  10\n",
      "Warning: No faces were detected.\n",
      "error in  ./data/s1/bbizzn.mpg  at frame  11\n"
     ]
    }
   ],
   "source": [
    "out, err = alignFace(vid_data, './data/s1/bbizzn.mpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
