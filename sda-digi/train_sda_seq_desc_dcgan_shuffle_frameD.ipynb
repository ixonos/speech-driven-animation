{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from sda.encoder_image import Encoder\n",
    "from sda.img_generator import Generator\n",
    "from sda.rnn_audio import RNN\n",
    "from sda.encoder_audio import Encoder as AEncoder\n",
    "\n",
    "from scipy import signal\n",
    "from skimage import transform as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import contextlib\n",
    "import shutil\n",
    "import skvideo.io as sio\n",
    "import scipy.io.wavfile as wav\n",
    "import ffmpeg\n",
    "import face_alignment\n",
    "from pydub import AudioSegment\n",
    "import pydub\n",
    "from pydub.utils import mediainfo\n",
    "\n",
    "import glob\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of real samples from video to check with discriminator\n",
    "D1_SAMPLES = 40\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "LR = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "BETA1 = 0.5\n",
    "\n",
    "# grab the names of the pre-processed video files\n",
    "video_filenames_proc = glob.glob('/home/jarrod/dev/speech-driven-animation/data/aligned_faces/*.npy')\n",
    "\n",
    "# delete bad entries\n",
    "bad_files = ['bgit2n', 'lbij7a', 'lrbr3s', 'pbio7a', 'sbaa4p', 'sgwp8p', 'sbim8p', \n",
    "             'srwi5a', 'swao7a', 'srbb4n', 'sbbbzp', 'lrae3s']\n",
    "\n",
    "for i,f in enumerate(video_filenames_proc):\n",
    "    \n",
    "    for bad in bad_files:\n",
    "        if f.find(bad) != -1:\n",
    "            del video_filenames_proc[i]\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getDataSample():\n",
    "    def __init__(self, model_path=\"grid\", gpu=-1):\n",
    "\n",
    "        self.audio_path = '/home/jarrod/dev/speech-driven-animation/data/npy_audio/'\n",
    "        self.device = torch.device(\"cuda:\" + str(gpu))\n",
    "\n",
    "        self.mean_face = np.load('./data/mean_face.npy')\n",
    "        self.img_size = (128,96)\n",
    "        self.aux_latent = 10\n",
    "        self.sequential_noise = True\n",
    "    \n",
    "        rnn_gen_dim = 256\n",
    "        id_enc_dim = 128\n",
    "        aud_enc_dim = 256\n",
    "        audio_feat_len = 0.2\n",
    "        self.audio_rate = 50000\n",
    "        self.video_rate = 25\n",
    "        self.audio_feat_samples = 10000\n",
    "        self.conversion_dict = {'s16': np.int16, 's32': np.int32}\n",
    "        \n",
    "        # image preprocessing\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((self.img_size[0], self.img_size[1])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    def preprocess_img(self, img):\n",
    "        src = self.fa.get_landmarks(img)[0][self.stablePntsIDs, :]\n",
    "        dst = self.mean_face[self.stablePntsIDs, :]\n",
    "        tform = tf.estimate_transform('similarity', src, dst)  # find the transformation matrix\n",
    "        warped = tf.warp(img, inverse_map=tform.inverse, output_shape=self.img_size)  # wrap the frame image\n",
    "        warped = warped * 255  # note output from wrap is double image (value range [0,1])\n",
    "        warped = warped.astype('uint8')\n",
    "\n",
    "        return warped\n",
    "\n",
    "    def __call__(self, filename, num_d1_samples, fs=None, aligned=False):\n",
    "        \n",
    "        audio_filename = self.audio_path + filename.split('/')[-1].split('.npy')[0] + '.wav.npy'\n",
    "        \n",
    "       \n",
    "        vid_data = np.load(filename)\n",
    "        frame = np.copy(vid_data[0])\n",
    "                \n",
    "        # take the input image and preprocess it    \n",
    "        frame = self.img_transform(frame)\n",
    "            \n",
    "        frame = frame.unsqueeze(0)\n",
    "        \n",
    "        # Load preprocessed audio data\n",
    "        audio_feat_seq = np.load(audio_filename)   \n",
    "        audio_feat_seq = torch.Tensor(audio_feat_seq)\n",
    "        audio_feat_seq_length = audio_feat_seq.shape[1]\n",
    "                \n",
    "        samp = np.random.rand(num_d1_samples)*audio_feat_seq_length\n",
    "        samp = samp.astype(int)\n",
    "\n",
    "        \n",
    "        normed_vid_data = []\n",
    "        \n",
    "        for i, img in enumerate(vid_data):\n",
    "            normed_vid_data.append(self.img_transform(img))\n",
    "        \n",
    "        normed_vid_data = torch.stack(normed_vid_data)\n",
    "        \n",
    "        d1_frames = normed_vid_data[samp]\n",
    "\n",
    "        # shuffle the id frames, otherwise the discriminator learns too fast    \n",
    "        np.random.shuffle(samp)\n",
    "        d1_frames_shuff = normed_vid_data[samp]\n",
    "\n",
    "        frame_c = frame.repeat(num_d1_samples,1,1,1) \n",
    "        # concat the id_frame onto the back of axis 1 (indices 3:)\n",
    "        d1_frames = torch.cat((d1_frames, d1_frames_shuff), axis=1)\n",
    "        \n",
    "        \n",
    "        return audio_feat_seq, audio_feat_seq_length, frame, normed_vid_data, d1_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class videoGenerator(nn.Module):\n",
    "    def __init__(self, gpu=-1):\n",
    "        \n",
    "        super(videoGenerator, self).__init__()\n",
    "            \n",
    "        self.device = torch.device(\"cuda:\" + str(gpu))    \n",
    "        \n",
    "        # size of noise vector\n",
    "        self.aux_latent = 10\n",
    "        self.sequential_noise = True\n",
    "        self.img_size = (128,96)\n",
    "        self.rnn_gen_dim = 256\n",
    "        self.id_enc_dim = 128\n",
    "        self.aud_enc_dim = 256\n",
    "        self.audio_feat_len = 0.2\n",
    "        self.audio_rate = 50000\n",
    "\n",
    "        # audio encoder\n",
    "        self.encoder = RNN(self.audio_feat_len, self.aud_enc_dim, self.rnn_gen_dim,\n",
    "                           self.audio_rate, init_kernel=0.005, init_stride=0.001)\n",
    "        \n",
    "\n",
    "        # id_image encoder\n",
    "        self.encoder_id = Encoder(self.id_enc_dim, self.img_size)\n",
    "        skip_channels = list(self.encoder_id.channels)\n",
    "        skip_channels.reverse()\n",
    "\n",
    "        # generator\n",
    "        self.generator = Generator(self.img_size, self.rnn_gen_dim, condition_size=self.id_enc_dim,\n",
    "                                   num_gen_channels=self.encoder_id.channels[-1],\n",
    "                                   skip_channels=skip_channels, aux_size=self.aux_latent,\n",
    "                                   sequential_noise=self.sequential_noise)\n",
    "        \n",
    "    def _broadcast_elements_(self, batch, repeat_no):\n",
    "        total_tensors = []\n",
    "        for i in range(0, batch.size()[0]):\n",
    "            total_tensors += [torch.stack(repeat_no * [batch[i]])]\n",
    "\n",
    "        return torch.stack(total_tensors)    \n",
    "    \n",
    "    \n",
    "    def forward(self, audio_feat_seq, audio_feat_seq_length, frame):\n",
    "        \n",
    "        # create audio encoding from the RNN\n",
    "        z = self.encoder(audio_feat_seq, [audio_feat_seq_length])  # Encoding for the motion\n",
    "        \n",
    "        # generate the noise input for the generator\n",
    "        noise = torch.FloatTensor(1, audio_feat_seq_length, self.aux_latent).normal_(0, 0.33).to(self.device)\n",
    "        \n",
    "        # create encoding from image\n",
    "        z_id, skips = self.encoder_id(frame, retain_intermediate=True)\n",
    "        \n",
    "        # the decoder is abstracted from the encoder, so pass the intermediate outputs of each\n",
    "        # conv layer (the skips) to the generator (which is the decoder)\n",
    "        skip_connections = []\n",
    "        for skip_variable in skips:\n",
    "            skip_connections.append(self._broadcast_elements_(skip_variable, z.size()[1]))\n",
    "        skip_connections.reverse()\n",
    "\n",
    "        z_id = self._broadcast_elements_(z_id, z.size()[1])\n",
    "        gen_video = self.generator(z, c=z_id, aux=noise, skip=skip_connections)\n",
    "        \n",
    "        return gen_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frameDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        # takes as input a tensor of shape [num frames, 6, height, width]\n",
    "        super(frameDiscriminator, self).__init__()\n",
    "        # number of channels = 3 id_frame, and 3 target frame\n",
    "        self.nc = 6\n",
    "        self.ndf = 64\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 128\n",
    "            nn.Conv2d(self.nc, self.ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 48 x 64\n",
    "            nn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "           \n",
    "            # state size. (ndf*2) x 24 x 32\n",
    "            nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 12 x 16\n",
    "            nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 6 x 8\n",
    "            nn.Conv2d(self.ndf * 8, self.ndf * 8, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.ndf*8*5*3, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "\n",
    "        x = x.view(-1, self.ndf*8*5*3)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return self.output(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seqDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        # takes as input a tensor of shape [num frames, 6, height, width]\n",
    "        super(seqDiscriminator, self).__init__()\n",
    "        # number of channels = 3 id_frame, and 3 target frame\n",
    "        self.nc_vid = 225\n",
    "        self.ndf_vid = 64\n",
    "        \n",
    "        self.vid = nn.Sequential(\n",
    "            # input is (nc) x 96 x 128\n",
    "            nn.Conv2d(self.nc_vid, self.ndf_vid, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 48 x 64\n",
    "            nn.Conv2d(self.ndf_vid, self.ndf_vid * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf_vid * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "           \n",
    "            # state size. (ndf*2) x 24 x 32\n",
    "            nn.Conv2d(self.ndf_vid * 2, self.ndf_vid * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf_vid * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf_vid*4) x 12 x 16\n",
    "            nn.Conv2d(self.ndf_vid * 4, self.ndf_vid * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ndf_vid * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf_vid*8) x 6 x 8\n",
    "            nn.Conv2d(self.ndf_vid * 8, self.ndf_vid * 8, 4, 1, 0, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.nc_aud = 75\n",
    "        self.ndf_aud = 64\n",
    "        \n",
    "        self.aud = nn.Sequential(\n",
    "            # input is (nc) x 10000 x 1\n",
    "            nn.Conv1d(self.nc_aud, self.ndf_aud, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf_aud) x 48 x 64\n",
    "            nn.Conv1d(self.ndf_aud, self.ndf_aud * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "           \n",
    "            # state size. (ndf_aud*2) x 24 x 32\n",
    "            nn.Conv1d(self.ndf_aud * 2, self.ndf_aud * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf_aud*4) x 12 x 16\n",
    "            nn.Conv1d(self.ndf_aud * 4, self.ndf_aud * 8, 5, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf_aud*8) x 6 x 8\n",
    "            nn.Conv1d(self.ndf_aud * 8, self.ndf_aud * 8, 5, 2, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(self.ndf_aud * 8, self.ndf_aud * 8, 5, 4, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(self.ndf_aud * 8, self.ndf_aud * 8, 3, 4, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(self.ndf_aud * 8, self.ndf_aud * 8, 3, 4, 1, bias=False),\n",
    "            nn.BatchNorm1d(self.ndf_aud * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(10240, 512),\n",
    "            nn.BatchNorm1d(1),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x_vid, x_aud):\n",
    "        x_vid = self.vid(x_vid)\n",
    "        x_aud = self.aud(x_aud)\n",
    "\n",
    "        x_aud = x_aud.unsqueeze(3)\n",
    "        \n",
    "        x_con = torch.cat((x_vid, x_aud), axis=3)\n",
    "\n",
    "        x_con = x_con.view(-1, 1, 512*5*4)\n",
    "\n",
    "        x_con = self.output(x_con).squeeze()\n",
    "        \n",
    "        return x_con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Networks and Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = getDataSample(gpu=0, model_path=\"grid\")\n",
    "\n",
    "frameD = frameDiscriminator().to(device)\n",
    "frameD.apply(weights_init)\n",
    "\n",
    "seqD = seqDiscriminator().to(device)\n",
    "seqD.apply(weights_init)\n",
    "\n",
    "netG = videoGenerator(gpu=0).to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD1 = optim.Adam(frameD.parameters(), lr=1e-4, betas=(BETA1, 0.999))\n",
    "optimizerD2 = optim.Adam(seqD.parameters(), lr=5e-5, betas=(BETA1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=2e-4, betas=(BETA1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    np.random.shuffle(video_filenames_proc)\n",
    "    \n",
    "    # For each batch in the dataloader\n",
    "    for i, filename in enumerate(video_filenames_proc):\n",
    "\n",
    "        size_test = np.load(filename)\n",
    "        if size_test.shape[0] < 75:\n",
    "            continue\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update id_frame D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "        frameD.zero_grad()\n",
    "        seqD.zero_grad()\n",
    "        \n",
    "        ## Train with all-real batch\n",
    "                \n",
    "        # Format batch\n",
    "        a1, a1_size, frame, vid, d1_frames = va(filename, D1_SAMPLES, aligned=True)\n",
    "        a1 = a1.to(device)\n",
    "        a1.requires_grad = True\n",
    "        frame = frame.to(device)\n",
    "        frame.requires_grad = True\n",
    "        d1_frames = d1_frames.to(device)\n",
    "        d1_frames.requires_grad = True\n",
    "        vid = vid.to(device)\n",
    "        vid.requires_grad = True\n",
    "        \n",
    "        # Forward pass real batch through frameD\n",
    "        frame_output = frameD(d1_frames).view(-1)\n",
    "        frameD_x = frame_output.mean().item()\n",
    "        \n",
    "        # Calculate loss on all-real batch\n",
    "        # we're checking d1_samples images for each forward pass\n",
    "        real_label = torch.full((D1_SAMPLES,), 1, device=device)\n",
    "        errD_real = criterion(frame_output, real_label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        \n",
    "    \n",
    "        ## Train with all-fake batch\n",
    "        \n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(a1, a1_size, frame)\n",
    "        \n",
    "        # don't want to descriminate all frames, just a subset\n",
    "        samp = np.random.rand(D1_SAMPLES)*a1_size\n",
    "        samp = samp.astype(int)\n",
    "        fake_samp = fake[samp]\n",
    "        real_samp = vid[samp]\n",
    "        \n",
    "        # concatenate id_frame\n",
    "#         frame_c = frame.repeat(fake_samp.shape[0],1,1,1)     \n",
    "        fake_samp = torch.cat((fake_samp, real_samp), axis=1)\n",
    "        \n",
    "        ### Classify all fake batch with frameD\n",
    "        frame_output = frameD(fake_samp.detach()).view(-1)\n",
    "        D_G_z1 = frame_output.mean().item()\n",
    "        \n",
    "        # Calculate frameD's loss on the all-fake batch\n",
    "        fake_label = torch.full((fake_samp.shape[0],), 0, device=device)\n",
    "        errD_fake = criterion(frame_output, fake_label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "       \n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        # Update D\n",
    "        optimizerD1.step()\n",
    "  \n",
    "        #\n",
    "        # Sequence discriminator\n",
    "        #\n",
    "    \n",
    "    \n",
    "        ## forward pass of real videos through seqD\n",
    "        a1 = a1.squeeze(3)\n",
    "        vid = vid.reshape(1,-1,128,96)\n",
    "       \n",
    "        \n",
    "        seq_output = seqD(vid, a1).view(-1)\n",
    "        seqD_x = seq_output.mean().item()\n",
    "        \n",
    "        # Calculate loss on all-real batch\n",
    "        seq_real_label = torch.full((1,), 0.75, device=device)\n",
    "        err_seqD_real = criterion(seq_output, seq_real_label)\n",
    "        \n",
    "        # Calculate gradients for D in backward pass\n",
    "        err_seqD_real *= 1\n",
    "        \n",
    "        err_seqD_real.backward()\n",
    "        \n",
    "\n",
    "        ### fake batch into seqD\n",
    "        \n",
    "        fake_vid = fake.reshape(1,-1,128,96)\n",
    "    \n",
    "        seq_output = seqD(fake_vid.detach(), a1).view(-1)\n",
    "        seqD_G_z1 = seq_output.mean().item()\n",
    "        \n",
    "        # Calculate seqD's loss on the all-fake batch\n",
    "        seq_fake_label = torch.full((1,), 0.25, device=device)\n",
    "        err_seqD_fake = criterion(seq_output, seq_fake_label)\n",
    "        err_seqD_fake *= 1\n",
    "        \n",
    "        # Calculate the gradients for this batch\n",
    "        err_seqD_fake.backward()\n",
    "        \n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        err_seqD = err_seqD_real + err_seqD_fake\n",
    "#         # Update D\n",
    "        optimizerD2.step()\n",
    "        \n",
    "        \n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = frameD(fake_samp).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        real_label = torch.full((fake_samp.shape[0],), 1, device=device)   \n",
    "        errG = criterion(output, real_label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        ## update G with sequencer gradient\n",
    "    \n",
    "        seq_output = seqD(fake_vid.detach(), a1).view(-1)\n",
    "        seq_real_label = torch.full((1,), 1, device=device)\n",
    "        err_seqG = criterion(seq_output, seq_real_label)\n",
    "        \n",
    "        err_seqG *= 1\n",
    "        err_seqG.backward()\n",
    "        \n",
    "        err_G_total = errG + err_seqG\n",
    "        \n",
    "        seqD_G_z2 = seq_output.mean().item()\n",
    "        \n",
    "        # turn dat crank\n",
    "        optimizerG.step()\n",
    "        \n",
    "#         seqD_x, seqD_G_z1, seqD_G_z2 = 0,0,0\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tframeD(x): %.4f\\tD(G(z)): %.4f / %.4f \\\n",
    "                  \\tseqD(x): %.4f\\tseqD(G(z)): %.4f / %.4f: '\n",
    "                  % (epoch, NUM_EPOCHS, i, len(video_filenames_proc),\n",
    "                     errD.item(), errG.item(), frameD_x, D_G_z1, D_G_z2, seqD_x, seqD_G_z1, seqD_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == NUM_EPOCHS-1) and (i == len(video_filenames_proc)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(a1, a1_size,frame).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "            plt.figure(figsize=(20,20))\n",
    "            plt.imshow(np.swapaxes(img_list[-1].T,0,1))\n",
    "            plt.show()\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), './model_dir/netG_10E')\n",
    "torch.save(frameD.state_dict(), './model_dir/frameD_10E')\n",
    "torch.save(seqD.state_dict(), './model_dir/seqD_10E')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list[0].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.swapaxes(img_list[3].T,0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a1_size, frame, vid, d1_frames = va(video_filenames_proc[0], D1_SAMPLES, aligned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a1.to(device)\n",
    "frame = frame.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = netG.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = netG(a1, a1_size, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_np = video.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = sio.FFmpegWriter(\"./tmp.avi\",\n",
    "                                      inputdict={'-r': str(25) + \"/1\", },\n",
    "                                      outputdict={'-r': str(25) + \"/1\", }\n",
    "                                      )\n",
    "for i in range(video.shape[0]):\n",
    "    frame = np.rollaxis(video_np[i, :, :, :], 0, 3)\n",
    "    frame = (frame/2+0.5)*255\n",
    "    frame = frame.astype(np.int)\n",
    "#     print(np.min(frame), np.max(frame))\n",
    "#     plt.imshow(frame)\n",
    "#     plt.show()\n",
    "\n",
    "#     if scale is not None:\n",
    "#         frame = tf.rescale(frame, scale, anti_aliasing=True, multichannel=True, mode='reflect')\n",
    "\n",
    "    writer.writeFrame(frame)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Cropping (Rough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the videos are 3s each, but the audio doesn't fill this timespan\n",
    "# may be necessary to crop the video to the audio, and re-align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '/home/jarrod/dev/speech-driven-animation/data/npy_audio/'\n",
    "\n",
    "audio_filename = audio_path + video_filenames_proc[0].split('/')[-1].split('.npy')[0] + '.wav.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filename,video_filenames_proc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in video_filenames_proc:\n",
    "    audio_filename = audio_path + f.split('/')[-1].split('.npy')[0] + '.wav.npy'\n",
    "    test = np.load(audio_filename)\n",
    "    print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_data = sio.vread(f.split('.wav')[0] + \".mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filenames = glob.glob('/home/jarrod/dev/speech-driven-animation/data/*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in audio_filenames:\n",
    "    \n",
    "    vid_data = sio.vread(f.split('.wav')[0] + \".mpg\")\n",
    "    \n",
    "    if vid_data.shape[0] != 75:\n",
    "    \n",
    "        print(f.split('.wav')[0] + \".mpg\")\n",
    "        print(vid_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = VideoFileClip(audio_filenames[1000].split('.wav')[0] + '.mpg')\n",
    "audio = video.audio\n",
    "# audio.write_audiofile('test.wav')\n",
    "test = audio.to_soundarray(fps=44100).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(test):\n",
    "    if np.abs(s) > 0.03:\n",
    "        left = i\n",
    "        break\n",
    "\n",
    "        \n",
    "for i, s in enumerate(test[::-1]):\n",
    "    if np.abs(s) > 0.03:\n",
    "        right = test.shape[0] - i\n",
    "        break\n",
    "\n",
    "num_frames = video.fps * video.duration    \n",
    "\n",
    "spf = num_frames / test.shape[0]\n",
    "fp_samp = test.shape[0] / num_frames\n",
    "\n",
    "# number of samples to move left or right to align with start of frame\n",
    "left_cut = left*spf - (((left*spf) % 1) * fp_samp) * spf\n",
    "right_cut = right*spf + ((1-(right*spf) % 1) * fp_samp) * spf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_cut, right_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
